\section{Introduction}
\label{introduction}

3D reconstruction of real-world scenes has a number of applications in diverse fields such as robotics, medical imaging, computer animation, computer graphics and computer vision. It is primarily a depth estimation problem that can be solved analytically with techniques such as clustering and segmentation, where input pixels in individual, or sequence of, images determine features such as edges and shades, from which volume and depth may be inferred. 3D reconstruction can also be achieved with the use of sensors such as LIDAR (Light Detection and Ranging). The output is a scaled volumetric representation, like a mesh or point cloud, defined as discrete representations of a continuous surface. 

Developments in artificial intelligence and machine learning have led to a class of learning algorithms known as CNNs (convolutional neural networks), successfully applied to image classification and natural language processing (NLP), initially, and now being applied to fields such as end-to-end autonomous driving and 3D reconstruction. 

Traditionally, computing algorithms have been "rules" based, where given an input, a sequence of instructions is executed producing an output. The term "end-to-end" when applied to machine learning algorithms, refers to processes with no such instructions, where, based on input data, a network is trained and "learns" parameters, to then be able to generate predictions based on new inputs.

3D reconstruction performed by neural networks is subject to similar constraints existing in 2D image classification and segmentation, where it is desirable that the algorithm is robust to occlusion, rotation, noise, etc. Concepts in 2D computer vision are also applicable to 3D, leading to common processes. There is a trend in autonomous vehicle research, to solve the 3D reconstruction problem, and indeed, the driving problem, with computer vision alone. Elon Musk, co-founder and CEO of Tesla, with respect to self-driving cars states that  "(...) right now AI and Neural Nets are used really for object recognition (...), identifying objects in still frames and tying it together in a perception path planning layer thereafter. (...) Over time I would expect that it moves really to (...) video in, car steering and pedals out" (\cite{TESLAADE:2019}). 

\cite{NguyenSurvey2013} presented a survey on 3D Point Cloud Segmentation, stating advances were made possible by the wide availability of LIDAR, and RGB-D (RGB plus depth) cameras such as Microsoft Kinect, and libraries such as the Point Cloud Library (\cite{pointcloud2011}), making point clouds more attractive to such fields as robotics. The references show the predominance of algorithmic approaches relying on feature engineering. A more recent survey (\cite{guo2019deep}) shows how deep neural networks in general and CNNs in particular have become widely used, at least in research, to address the problem of 3D computer vision. Though designing effective neural networks is an empirical process and, like algorithmic counterparts relying on engineered features, requires domain knowledge.

Based on this scenario, our \textbf{research question} is \textit{can accurate point clouds be generated from single images}, the \textbf{purpose of this research} is \textit{to find evidence to inform practice} (\cite{Oates:2006}) on point cloud generation through a CNN computer vision solution. We aim to 1. segment and classify the objects of interest within the image and 2. generate a point cloud for each object of interest that may be used in addition to, or in conjunction with, LIDAR point clouds. The \textbf{product of this research} is expected to be a model able to generate point clouds from single images and the \textbf{intended beneficiary} is primarily the author, hoping this work will create academic and professional opportunities, as well as anyone else researching in the area that may use this work as a stepping stone or starting point.

